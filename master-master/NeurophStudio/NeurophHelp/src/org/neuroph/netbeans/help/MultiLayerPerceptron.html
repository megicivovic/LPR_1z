<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<title>Multi Layer Perceptron</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body style="font-family:Tahoma;font-size:11px;">
    <h2>MULTI LAYER PERCEPTRON</h2>
    <p>Multi Layer perceptron (MLP) is a feedforward
      neural network with one or more
       layers between input and output layer. Feedforward means that data flows in one direction from input to output layer (forward). This type of network is  trained with the backpropagation learning algorithm.   MLPs are widely used for pattern
      classification, recognition, prediction and approximation. Multi Layer Perceptron can solve problems which are not linearly separable.</p>
    <p><img src="images/MLP.jpg" width="362" height="250"></p>
    <p>To create and train Multi Layer Perceptron
      neural network using  Neuroph Studio<i></i><i> </i>do the following:&nbsp;</p>
    <ol>
      <li >Create Neuroph project</li>
      <li >Create Multi Layer perceptron network</li>
      <li >Create training set</li>
      <li >Train network </li>
      <li >Test trained network</li>
    </ol>
<p><strong>Step 1.</strong> Create <strong>Neuroph project.</strong></p>
    Click File &gt; New Project.
    <p><img src="images/new project 1.png"></p>
<p>&nbsp;</p>
    Select Neuroph Project, and click Next.
    <p><img src="images/new project 2.png"></p>
    <p>&nbsp;</p>
    <p>Enter project name and location, click Finish. </p>
<p><img src="images/new project 3.png"></p>
    <p>Project is created, now create neural network. <br />
    </p>
    <p><strong>Step 2.</strong> Create <strong>Multilayer Perceptron</strong> network.</p>
    <p>Click File &gt; New File
    </p>
<p><img src="images/new file.png"></p>
<p>&nbsp;</p>
    Select project from Project drop-down menu, select Neural Network file type, click next.
<p><img src="images/new network.png"></p>
    <p>&nbsp;</p>
    <p>Enter network name, select Multi Layer Perceptron network type, click next. </p>
<p><img src="images/mlp 001.png"></p>
  <p>&nbsp;</p>
    <p>Enter number of
  input neurons (2), number of hidden neurons (3) and number of output neurons (1) in each layer. Leave the <i>Use Bias Neurons</i> box     checked. Choose transfer function Sigmoid or Tanh from drop down menu, for learning rule choose <b>Backpropagation</b> and click <b>Create</b> button. </p>
<p><img src="images/mlp 002.png"></p>
    <p>&nbsp;</p>
    <p>&nbsp;This will create the Multi Layer Perceptron
      neural network with two neurons in input, three in hidden and one in output
    layer. All neurons will have <b>Tanh</b> transfer functions.</p>
<p><img src="images/mlp 003.png"></p>
    <p>&nbsp;Now
      we shall train this network to learn logical XOR function. We'll create new training set
      according to XOR truth table.</p>
    <p><strong>Step 3.</strong>Â Next, create training set. In main menu click <b>File
&gt; New File</b> to open training set wizard.</p>
    <p><img src="images/new file.png"></p>
<p>&nbsp;</p>
    Select Data set file type, then click next.
<p><img src="images/new training set.png"></p>
<p>&nbsp;</p>
<p>Enter training set name, choose Supervised for training set type from drop down list, enter number of inputs and outputs as shown on picture below and click <b>Next </b>button. </p>
    <p><img src="images/mlp 004.png"></p>
<p>&nbsp;</p>
    <p>Then create training set by entering training
      elements as input and desired output values for neurons in input and output
  layer respectively. Use <b>Add row</b> button to add new elements, and click <b>OK</b> button when finished.</p>
    <p><img src="images/mlp 005.png"></p>
<p>&nbsp;</p>
    <p><strong>Step 4.</strong> Training network.To start network
      training procedure, drag n' drop training set to corresponding field in the network window, 
      and 'Train' button will become enabled in toolbar. Click the 'Train' button to open Set Learning Parameters dialog. </p>
    <p><img src="images/mlp 006.png"></p>
<p>&nbsp;</p>
    <p>In <b>Set Learning parameters </b>dialog
  use default learning parameters, and just click the <b>Train</b> button.</p>
    <p><img src="images/mlp 007.png"></p>
<p>&nbsp;</p>
<p>Training stopped after 1700 iterations with total net error under 0.01. Try using different transfer function and learning rate and observe the results.</p>
    <p><img src="images/mlp 008.png"></p>
    <p><b>Step 5.</b> Testing trained network. After the training is complete, you can use
<b>Test</b> and <b>SetIn</b> buttons to test the network behaviour. </p>
    <p><img src="images/mlp 009.png"></p>
    <p><img src="images/mlp 010.png"></p>
<p>&nbsp;</p>
    <p>&nbsp;In <b>Set Network Input Dialog</b> you can enter input values for network separated with spaces.
</p>
    <p><img src="images/mlp 011.png"></p>
    <p>&nbsp;The result of the network test is shown on
      picture below. </p>
    <p><img src="images/mlp 013.png"></p>
    <p><b>&nbsp;</b>Value of output
      neuron is close to 1, which is the desired output for the given input. The small difference represents the acceptable error. </p>
    <p>&nbsp;Right click editor and choose <b>Display Properties</b> to set different visualization options (see picture below).</p>
    <p><img src="images/mlp 014.png"></p>
<p>&nbsp;</p>

<p>You can now experiment with different combinations of transfer function and learning rule, while creating new MLP network.
For example, choose <b>Backpropagation with Momentum</b> for <b> Learning Rule</b>.</p> 
<p><img src="images/mlp 015.png"></p>
    <p>Create the same training set (according to XOR truth table) and click train button.</p>
	<p><img src="images/mlp 006.png"></p>
    <p>In <b>Set Learning parameters </b>dialog for <b>Momentum</b> use 0.1, and click the <b>Train</b> button.</p>
    <p><img src="images/mlp 016.png"></p>
    <p>Training stopped after just 32 iterations with total net error under 0.01</p>
    <p><img src="images/mlp 017.png"></p>
    <p>Test network using Set Input for the same values (0 1) as in previous example.</p>
    <p>&nbsp;The result of network test is shown on picture below. </p>
    <p><img src="images/mlp 018.png"></p>
    <p><b>&nbsp;</b>Value of output
      neuron is close to 1, which is the desired output for the given input. The small difference represents the acceptable error. </p>

    <p>&nbsp;</p>
    <h3>MULTI LAYER PERCEPTRON IN JAVA CODE </h3>
    <p>package org.neuroph.samples;</p>
    <p>import java.util.Arrays;<br>
      import org.neuroph.core.NeuralNetwork;<br>
      import org.neuroph.nnet.MultiLayerPerceptron;<br>
      import org.neuroph.core.data.DataSet;<br>
      import org.neuroph.core.data.DataSetRow;<br>
      import org.neuroph.util.TransferFunctionType;</p>
    <p>/**<br>
      * This sample shows how to create, train, save and load simple Multi Layer Perceptron<br>
      */<br>
      public class XorMultiLayerPerceptronSample {</p>
    <blockquote>
      <p> public static void main(String[] args) {</p>
      <blockquote>
        <p>        // create training set (logical XOR function)<br>
          DataSet trainingSet = new DataSet(2, 1);<br>
          trainingSet.addRow(new DataSetRow(new double[]{0, 0}, new double[]{0}));<br>
          trainingSet.addRow(new DataSetRow(new double[]{0, 1}, new double[]{1}));<br>
          trainingSet.addRow(new DataSetRow(new double[]{1, 0}, new double[]{1}));<br>
          trainingSet.addRow(new DataSetRow(new double[]{1, 1}, new double[]{0}));</p>
        <p> // create multi layer perceptron<br>
          MultiLayerPerceptron myMlPerceptron = new MultiLayerPerceptron(TransferFunctionType.TANH, 2, 3, 1);<br>
          // learn the training set<br>
        myMlPerceptron.learn(trainingSet);</p>
        <p> // test perceptron<br>
          System.out.println(&quot;Testing trained neural network&quot;);<br>
          testNeuralNetwork(myMlPerceptron, trainingSet);</p>
        <p> // save trained neural network<br>
          myMlPerceptron.save(&quot;myMlPerceptron.nnet&quot;);</p>
        <p> // load saved neural network<br>
          NeuralNetwork loadedMlPerceptron = NeuralNetwork.createFromFile(&quot;myMlPerceptron.nnet&quot;);</p>
        <p> // test loaded neural network<br>
          System.out.println(&quot;Testing loaded neural network&quot;);<br>
          testNeuralNetwork(loadedMlPerceptron, trainingSet);</p>
      </blockquote>
      <p>        }</p>
      <p> public static void testNeuralNetwork(NeuralNetwork nnet, DataSet testSet) {</p>
      <blockquote>
         for(DataSetRow dataRow : testSet.getRows()) {
        <blockquote>
            nnet.setInput(dataRow.getInput());<br>
            nnet.calculate();<br>
            double[ ] networkOutput = nnet.getOutput();<br>
System.out.print(&quot;Input: &quot; + Arrays.toString(dataRow.getInput()) );<br>
System.out.println(&quot; Output: &quot; + Arrays.toString(networkOutput) );
        </blockquote>
        <p>        }</p>
      </blockquote>
      <p>        }</p>
      <p>&nbsp;</p>
    </blockquote>
    <p>}<br>
    </p>
    <h3>EXTERNAL LINKS </h3>
    <p>To learn more about the Multi Layer Perceptrons and Backpropagation (learning rule for Multi Layer Perceptron) see:<br>
      <br>
<a href="http://www.learnartificialneuralnetworks.com/backpropagation.html ">http://www.learnartificialneuralnetworks.com/backpropagation.html </a><br>
<a href="http://en.wikipedia.org/wiki/Backpropagation">http://en.wikipedia.org/wiki/Multilayer_perceptron</a> <br>
<a href="http://en.wikipedia.org/wiki/Backpropagation">http://en.wikipedia.org/wiki/Backpropagation</a>                     
  

</p>
    <p>&nbsp;</p>
</body>
</html>